<!DOCTYPE html>
<html lang="en"><!-- InstanceBegin template="/Templates/MainTemplate.dwt" codeOutsideHTMLIsLocked="false" -->

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">
    <!-- InstanceBeginEditable name="doctitle" -->
    <title>Scalable Rendering</title>
    <!-- InstanceEndEditable -->

    <!-- Custom CSS -->
    <link href="../assets/css/kmorel.css" rel="stylesheet">
    <link href="../assets/css/simple-sidebar.css" rel="stylesheet">
    <link href="../assets/css/slideshow.css" rel="stylesheet">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

<!-- InstanceBeginEditable name="head" -->
<!-- InstanceEndEditable -->
</head>

<body>

    <!-- jQuery -->
    <script src="../assets/js/jquery.js"></script>

    <div id="body-wrapper">

        <!-- Sidebar -->
        <div id="sidebar-wrapper">
            <ul class="sidebar-nav">
                <li class="sidebar-brand"><a href="../index.html">Kenneth Moreland</a></li>
                <li><a href="../projects.html">Projects</a></li>
                <li><a href="../highlights.html">Research Highlights</a></li>
                <ul class="sidebar-sub">
                    <li><a href="../color-advice/">Color Map Advice</a></li>
                    <li><a href="../parallel-scaling-metrics/">Parallel Scaling Metrics</a></li>
                    <li><a href="../color-maps/">Diverging Color Maps</a></li>
                    <li><a href="../topology-threading/">Topology Threading</a></li>
                    <li><a href="../vis-pipelines/">Visualization Pipelines</a></li>
                    <li><a href="../scalable-rendering/">Scalable Rendering</a></li>
                    <li><a href="../fftgpu/">The FFT on a GPU</a></li>
                    <li><a href="../partial-pre-integration/">Partial Pre-Integration</a></li>
                </ul>
                <li>
                    <a href="../publications.html">Publications</a>
                </li>
                <ul class="sidebar-sub">
                    <li><a href="../publications.html#journal">Journals and Conferences</a></li>
                    <li><a href="../publications.html#dissertation">Ph.D. Dissertation</a></li>
                    <li><a href="../publications.html#tier-3">Symposiums and Workshops</a></li>
                    <li><a href="../publications.html#tech-report">Technical Reports</a></li>
                    <li><a href="../publications.html#poster">Posters</a></li>
                    <li><a href="../publications.html#presentation">Presentations</a></li>
                    <li><a href="../publications.html#panels">Panels</a></li>
                </ul>
                <li><a href="../professional-activities.html">Professional Activities</a></li>
                <li><a href="../education.html">Education</a></li>
                <li><a href="../contact.html">Contact</a></li>
            </ul>
        </div>
        <!-- /#sidebar-wrapper -->

        <!-- Page Content -->
        <div id="page-content-wrapper">
            <div id="page-title">
                <h1><!-- InstanceBeginEditable name="PageTitle" -->Large Scale Parallel Rendering<!-- InstanceEndEditable --></h1>
            </div>
            <div id="page-body"><!-- InstanceBeginEditable name="PageBody" -->
              <p><img src="../images/thumbnails/scalable-rendering.png" alt="" width="200" height="135" class="pubImage"/>In this work we explore large-scale parallel rendering on HPC systems. The major deployment platform for this work is the scalable rendering library <a href="http://icet.sandia.gov/" target="_blank">IceT</a>, which is made freely available. Our main approach is a technique known as sort-last parallel rendering, but along the way we introduce several simple to implement but powerful modifications that greatly improve the efficiency including minimal copy image interlacing for better load balancing and telescoping compositing for arbitrary job sizes. Visit the <a href="http://icet.sandia.gov/" target="_blank">IceT</a> project page for access to the software, documentation, and further papers and information on scalable rendering. For users more interested in easy to read and edit code than high efficiency, we also provide the <a href="https://github.com/sandialabs/miniGraphics" target="_blank">miniGraphics</a> miniapp.</p>
              <h2 id="LDAV2021">GPU-based Compression for Distributed Rendering</h2>
              <p>&quot;<span class="pubTitle">GPU-based Image Compression for Efficient Compositing in Distributed Rendering Applications</span>.&quot; Riley Lipinksi, Kenneth Moreland, Michael E. Papka, Thomas Marrinan. In <em>Proceedings of the 11th IEEE Symposium on Large Data Analysis and Visualization (LDAV)</em>. October 2021. DOI <a _ngcontent-yio-c184="" append-to-href="?src=document" target="_blank" href="https://doi.org/10.1109/LDAV53230.2021.00012">10.1109/LDAV53230.2021.00012</a>.</p>
              <h3>Abstract</h3>
              <p>Visualizations of large-scale data sets are often created on graphics clusters that distribute the rendering task amongst many processes. When using real-time GPU-based graphics algorithms, the most time-consuming aspect of distributed rendering is typically the com-positing phase - combining all partial images from each rendering process into the final visualization. Compo siting requires image data to be copied off the GPU and sent over a network to other processes. While compression has been utilized in existing distributed rendering compositors to reduce the data being sent over the network, this compression tends to occur after the raw images are transferred from the GPU to main memory. In this paper, we present work that leverages OpenGL / CUDA interoperability to compress raw images on the GPU prior to transferring the data to main memory. This approach can significantly reduce the device-to-host data transfer time, thus enabling more efficient compositing of images generated by distributed rendering applications.</p>
              <h3>Full Paper</h3>
              <p><a href="icet-gpu-compression.pdf"><span class="pubTitle">GPU-based Image Compression for Efficient Compositing in Distributed Rendering Applications</span></a></p>
              <h2 id="LDAV2018">Binary-Swap on Odd Factors of Processes</h2>
              <p>&quot;<span class="pubTitle">Comparing Binary-Swap Algorithms for Odd Factors of Processes</span>.&quot; Kenneth Moreland. In <em>Proceedings of the 8th IEEE Symposium on Large Data Analysis and Visualization (LDAV)</em>. October 2018. DOI <a href="https://doi.org/10.1109/LDAV.2018.8739210" target="_blank">10.1109/LDAV.2018.8739210</a>.</p>
              <h3>Abstract</h3>
              <p> A key component of most large-scale rendering systems is a parallel image compositing algorithm, and the most commonly used compositing algorithms are binary swap and its variants. Although shown to be very efficient, one of the classic limitations of binary swap is that it only works on a number of processes that is a perfect power of 2. Multiple variations of binary swap have been independently introduced to overcome this limitation and handle process counts that have factors that are not 2. To date, few of these approaches have been directly compared against each other, making it unclear which approach is best. This paper presents a fresh implementation of each of these methods using a common software framework to make them directly comparable. These methods to run binary swap with odd factors are directly compared. The results show that some simple compositing approaches work as well or better than more complex algorithms that are more difficult to implement.</p>
              <h3>Full Paper</h3>
              <p><a href="BinarySwapNon2.pdf"><span class="pubTitle">Comparing Binary-Swap Algorithms for Odd Factors of Processes</span></a></p>
              <h3>Supplemental Material</h3>
              <p>Here are <a href="https://1drv.ms/p/s!Aub-LzOy6dCvgWWI3myPagTlEKGs">the slides used to present this material</a>. The slides really need a narrator to explain the algorithms, but they have nice animations that help describe the content.</p>
              <p>The software used for testing in this paper is being made available in the <a href="https://github.com/sandialabs/miniGraphics" target="_blank">miniGraphics</a> miniapp, which is released as open-source software and is made available through <a href="https://github.com/sandialabs/miniGraphics" target="_blank">a GitHub repository</a>. The experiments were specifically performed with the repository at SHA <a href="https://github.com/sandialabs/miniGraphics/tree/50630f94848b970cc8894eb0ceb84b47bcef20f5">50630f9</a> with the exception of the 234-composite algorithm (which was implemented afterward). These use the code at SHA <a href="https://github.com/sandialabs/miniGraphics/tree/4e647c2bb7fe5b77a1d5340c3480b146dcbf3947">4e647c2</a>. If you really want to replicate the exact same code for reproducibility, you can <a href="https://github.com/sandialabs/miniGraphics/archive/4e647c2bb7fe5b77a1d5340c3480b146dcbf3947.zip">download this exact version</a>. For most purposes, though, you are probably better off with <a href="https://github.com/sandialabs/miniGraphics" target="_blank">the latest version of miniGraphics</a>.</p>
              <p>For those interested in exploring the data collected by the reported experiments more thoroughly, I also post all the log files collected through the experiments here. The <a href="https://github.com/sandialabs/miniGraphics" target="_blank">miniGraphics</a> application writes out its timing measurements in yaml format. Because these timing logs can get verbose, each are compressed with gzip.</p>
              <ul>
                <li><a href="https://github.com/kmorel-papers/BinarySwapNon2/raw/master/data/miniGraphics-skybridge-vn-scaling.yaml.gz">miniGraphics-skybridge-vn-scaling.yaml.gz</a> Measurements of the system when running in &quot;virtual node&quot; mode. This is where most of the data from Section 3.1 comes from.</li>
                <li><a href="https://github.com/kmorel-papers/BinarySwapNon2/raw/master/data/miniGraphics-skybridge-vn-corrections.yaml.gz">miniGraphics-skybridge-vn-corrections.yaml.gz</a> As noted in Section 3.4.3, a few inconsistent readings are observed. These are likely due to aberrant system slowdowns and so were re-run. The measurements in this file replace those in miniGraphics-skybridge-vn-scaling.yaml.gz.</li>
                <li><a href="https://github.com/kmorel-papers/BinarySwapNon2/raw/master/data/miniGraphics-skybridge-vn-no-compress.yaml.gz">miniGraphics-skybridge-vn-no-compress.yaml.gz</a> A subset of measurements run without using image compression. These results are reported in Section 3.2.</li>
                <li><a href="https://github.com/kmorel-papers/BinarySwapNon2/raw/master/data/miniGraphics-skybridge-smp-scaling.yaml.gz">miniGraphics-skybridge-smp-scaling.yaml.gz</a> Similar to the &quot;virtual node&quot; scaling studies, these experiments are run in &quot;pure distributed&quot; mode where every MPI process was on a separate compute node. These results are reported in Section 3.3 (although many of these measurements are not reported in the paper due to limited space).</li>
              </ul>
              <p>The figures in the paper are created in Python using <a href="https://pandas.pydata.org/" target="_blank">pandas</a> and <a href="https://toyplot.readthedocs.io" target="_blank">toyplot</a> modules. I also execute the Python script in <a href="http://jupyter.org/" target="_blank">Jupyter notebooks</a>. These notebooks are provided for you to either directly run, copy into your own scripts, or use as inspiration for whatever processing you prefer.</p>
              <ul>
                <li><a href="https://github.com/kmorel-papers/BinarySwapNon2/blob/master/data/scaling-plots.ipynb">scaling-plots.ipynb</a> The basic scaling plots generating the first two figures of Section 3.1.</li>
                <li><a href="https://github.com/kmorel-papers/BinarySwapNon2/blob/master/data/2-3-swap-overhead.ipynb">2-3-swap-overhead.ipynb</a> A plot of the overhead of building the composite tree for the 2-3 swap algorithm. Creates the last figure of Section 3.1.</li>
                <li><a href="https://github.com/kmorel-papers/BinarySwapNon2/blob/master/data/no-image-compress.ipynb">no-image-compress.ipynb</a> The scaling plots when not using image compression. Creates the figures of Section 3.2.</li>
                <li><a href="https://github.com/kmorel-papers/BinarySwapNon2/blob/master/data/vn-vs-smp.ipynb">vn-vs-smp.ipynb</a> Plots comparing the behavior of virtual node vs. pure virtual modes. Creates the figures of Section 3.3.</li>
              </ul>
              <h2 id="IceTAtScale">An Image Compositing Solution at Scale</h2>
              <p>&quot;<span class="pubTitle">An Image Compositing Solution at Scale</span>.&quot; Kenneth Moreland, Wesley Kendall, Tom Peterka, and Jian Huang. In<em> Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis (SC '11)</em>. November 2011. DOI&nbsp;<a href="http://dx.doi.org/10.1145/2063384.2063417" target="_blank">10.1145/2063384.2063417</a>.</p>
              <h3>Abstract</h3>
              <p>The only proven method for performing distributed-memory parallel rendering at large scales, tens of thousands of nodes, is a class of algorithms called sort last. The fundamental operation of sort-last parallel rendering is an image composite, which combines a collection of images generated independently on each node into a single blended image. Over the years numerous image compositing algorithms have been proposed as well as several enhancements and rendering modes to these core algorithms. However, the testing of these image compositing algorithms has been with an arbitrary set of enhancements, if any are applied at all. In this paper we take a leading production-quality image compositing framework, <a href="http://icet.sandia.gov/" target="_blank">IceT</a>, and use it as a testing framework for the leading image compositing algorithms of today. As we scale IceT to ever increasing job sizes, we consider the image compositing systems holistically, incorporate numerous optimizations, and discover several improvements to the process never considered before. We conclude by demonstrating our solution on 64K cores of the Intrepid BlueGene/P at Argonne National Laboratories.</p>
              <h3>Full Paper</h3>
              <p><a href="https://dl.acm.org/doi/10.1145/2063384.2063417?cid=81100570201"><span class="pubTitle">An Image Compositing Solution at Scale</span></a></p>
              <h3>Supplemental Material</h3>
              <p>You can download <a href="IceTAtScaleArtifacts.tar.bz2">the artifacts generated for this paper</a>. This is a collection of the raw timing data collected during the scaling studies.</p>
              <h2 id="PVG2001">Sort-Last Parallel Rendering on Tile Displays</h2>
            <p>&quot;<span class="pubTitle">Sort-Last Parallel Rendering for Viewing Extremely Large Data Sets on Tile Displays</span>.&quot; Kenneth Moreland, Brian Wylie, and Constantine Pavlakos. In <em>Proceedings of IEEE 2001 Symposium on Parallel and Large-Data Visualization and Graphics</em>, October 2001, pp. 85&ndash;92.</p>
              <h3>Abstract</h3>
              <p>Due to the impressive price-performance of today&rsquo;s PC- based graphics accelerator cards, Sandia National Laboratories is attempting to use PC clusters to render extremely large data sets in interactive applications. This paper describes a sort-last parallel rendering system running on a PC cluster that is capable of rendering enormous amounts of geometry onto high-resolution tile displays by taking advantage of the spatial coherency that is inherent in our data. Furthermore, it is capable of scaling to larger sized input data or higher resolution displays by increasing the size of the cluster. Our prototype is now capable of rendering 120 million triangles per second on a 12 mega-pixel display. </p>
            <h3>Full Paper</h3>
            <p><a href="PVG2001.pdf"><span class="pubTitle">Sort-Last Parallel Rendering for Viewing Extremely Large Data Sets on Tile Displays</span></a> </p>
          <!-- InstanceEndEditable --></div>
            <div class="imageCredit">SAND 2015-4836 W</div>
        </div>
        <!-- /#page-content-wrapper -->

    </div>
    <!-- /#wrapper -->

</body>

<!-- InstanceEnd --></html>
